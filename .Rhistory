Mid_Century_Maximum = max(midcent_median_50yr, na.rm = TRUE)
) %>%
# Reshape data for output
tidyr::pivot_longer(cols = everything(), names_to = "Measure", values_to = "Value") %>%
tidyr::separate(Measure, into = c("Period", "Statistic"), sep = "_") %>%
tidyr::spread(key = Period, value = Value)
data <- read_excel("AT&T Public_Climate_Data_2022/Wildfire_Extremes_Historic_and_Midcentury.csv
")
data <- read_csv("AT&T Public_Climate_Data_2022/Wildfire_Extremes_Historic_and_Midcentury.csv
")
library(reader)
library(readr)
data <- read_csv("AT&T Public_Climate_Data_2022/Wildfire_Extremes_Historic_and_Midcentury.csv
")
summary(data)
library(readr)
data <- read_csv("AT&T Public_Climate_Data_2022/Wildfire_Extremes_Historic_and_Midcentury.csv")
summary(data)
data<- read_csv("AT&T Public_Climate_Data_2022/Wind_Extremes_Historic_and_Midcentury.csv")
summary(data)
data <- read_excel("AT&T Public_Climate_Data_2022/Drought_Extremes_Historic_and_Midcentury.xlsx")
summary(data)
nrwo(data)
data <- read_excel("AT&T Public_Climate_Data_2022/Drought_Extremes_Historic_and_Midcentury.xlsx")
summary(data)
nrow(data)
# Alternatively, for a more detailed exploration, you can use dplyr to summarise each colum
library(readr)
data <- read_csv("AT&T Public_Climate_Data_2022/Wildfire_Extremes_Historic_and_Midcentury.csv")
summary(data)
nrow(data)
data<- read_csv("AT&T Public_Climate_Data_2022/Wind_Extremes_Historic_and_Midcentury.csv")
summary(data)
nrow(data)
missing_coords <- sum(is.na(data))
print(paste("Missing coordinate pairs:", missing_coords))
library(ggplot2)
library(maps)
library(ggplot2)
install.packages("maps")
library(maps)
ggplot(data, aes(x = Longitude, y = Latitude)) +
borders("state") +  # for USA state borders
geom_point(alpha = 0.5) +
coord_fixed(1.3) +
theme_minimal()
library(ggplot2)
library(maps)
ggplot(data, aes(x = Longitude, y = Latitude)) +
borders("state") +  # for USA state borders
geom_point(alpha = 0.5) +
coord_fixed(1.3) +
theme_minimal()
library(ggplot2)
library(maps)
us_map <- map_data("state")  # Get US states map data
gg <- ggplot() +
geom_polygon(data = us_map, aes(x = long, y = lat, group = group),
fill = "white", color = "black", size = 0.25) +
coord_fixed(1.3) +
theme_minimal()
# Assuming 'data' is your dataset with Latitude and Longitude columns
gg <- gg + geom_point(data = data, aes(x = Longitude, y = Latitude),
color = "red", alpha = 0.5, size = 1)
library(ggplot2)
library(maps)
us_map <- map_data("state")  # Get US states map data
gg <- ggplot() +
geom_polygon(data = us_map, aes(x = long, y = lat, group = group),
fill = "white", color = "black", size = 0.25) +
coord_fixed(1.3) +
theme_minimal()
# Assuming 'data' is your dataset with Latitude and Longitude columns
gg <- gg + geom_point(data = data, aes(x = Longitude, y = Latitude),
color = "red", alpha = 0.5, size = 1)
print(gg)
library(ggplot2)
library(maps)
us_map <- map_data("state")  # Get US states map data
gg <- ggplot() +
geom_polygon(data = us_map, aes(x = long, y = lat, group = group),
fill = "white", color = "black", size = 0.25) +
coord_fixed(1.3) +
theme_minimal()
# Assuming 'data' is your dataset with Latitude and Longitude columns
gg <- gg + geom_point(data = data, aes(x = Longitude, y = Latitude),
color = "red", alpha = 0.5, size = 1)
# Example: Highlighting Florida, Georgia, North Carolina, South Carolina
covered_states <- c("florida", "georgia", "north carolina", "south carolina")
us_map$covered <- ifelse(us_map$region %in% covered_states, "covered", "not covered")
gg <- gg + geom_polygon(data = us_map, aes(fill = covered),
color = "black", size = 0.25)
gg <- gg + scale_fill_manual(values = c("covered" = "blue", "not covered" = "gray70"))
gg <- gg + labs(title = "Data Coverage Map",
subtitle = "Coverage of Wildfire, Drought, and Wind Speeds Data",
x = "Longitude", y = "Latitude")
print(gg)
library(ggplot2)
library(maps)
# Load US state map data
us_map <- map_data("state")
# Define the covered states
covered_states <- c("florida", "georgia", "north carolina", "south carolina")
us_map$covered <- ifelse(us_map$region %in% covered_states, "covered", "not covered")
# Create the base map with state borders and highlighted states
gg <- ggplot(data = us_map, aes(x = long, y = lat, group = group)) +
geom_polygon(aes(fill = covered), color = "black", size = 0.25) +
scale_fill_manual(values = c("covered" = "blue", "not covered" = "gray70")) +
coord_fixed(1.3) +
theme_minimal() +
labs(title = "Data Coverage Map",
subtitle = "Coverage of Wildfire, Drought, and Wind Speeds Data",
x = "Longitude", y = "Latitude")
# Assuming 'data' is your dataset with Latitude and Longitude columns
# Add your data points
gg <- gg + geom_point(data = data, aes(x = Longitude, y = Latitude),
color = "red", alpha = 0.5, size = 1)
# Print the map
print(gg)
library(ggplot2)
library(maps)
# Load US state map data
us_map <- map_data("state")
# Define the covered states
covered_states <- c("florida", "georgia", "north carolina", "south carolina")
us_map$covered <- ifelse(us_map$region %in% covered_states, "covered", "not covered")
# Create the base map with state borders and highlighted states
gg <- ggplot() +
geom_polygon(data = us_map, aes(x = long, y = lat, group = group, fill = covered), color = "black", size = 0.25) +
scale_fill_manual(values = c("covered" = "blue", "not covered" = "gray70")) +
coord_fixed(1.3) +
theme_minimal() +
labs(title = "Data Coverage Map",
subtitle = "Coverage of Wildfire, Drought, and Wind Speeds Data",
x = "Longitude", y = "Latitude")
# Assuming 'data' is your dataset with Latitude and Longitude columns
# Add your data points with correct aesthetic mappings specific to this layer
gg <- gg + geom_point(data = data, aes(x = Longitude, y = Latitude),
color = "red", alpha = 0.5, size = 1)
# Print the map
print(gg)
library(ggplot2)
library(maps)
# Load US state map data
us_map <- map_data("state")
# Define the covered states
covered_states <- c("florida", "georgia", "north carolina", "south carolina")
us_map$covered <- ifelse(us_map$region %in% covered_states, "covered", "not covered")
# Create the base map with state borders and highlighted states
gg <- ggplot() +
geom_polygon(data = us_map, aes(x = long, y = lat, group = group, fill = covered), color = "black", size = 0.25) +
scale_fill_manual(values = c("covered" = "blue", "not covered" = "gray70")) +
coord_fixed(1.3) +
theme_minimal() +
labs(title = "Data Coverage Map",
subtitle = "Coverage of Wildfire, Drought, and Wind Speeds Data",
x = "Longitude", y = "Latitude")
# Assuming 'data' is your dataset with Latitude and Longitude columns
# Add your data points with correct aesthetic mappings specific to this layer
gg <- gg + geom_point(data = data, aes(x = Longitude, y = Latitude),
color = "blue", alpha = 0.5, size = 1)
# Print the map
print(gg)
library(ggplot2)
library(maps)
library(mapproj)
library(ggplot2)
library(maps)
install.packages("mapproj")
library(mapproj)
# Example data frame with Longitude and Latitude
# Ensure to replace this with your actual dataset
data <- data.frame(
Longitude = c(-149.9003, -155.6659, -77.0369),  # Example: Alaska, Hawaii, and Washington D.C.
Latitude = c(61.2181, 19.8968, 38.9072)
)
# Get map data for the USA including Alaska and Hawaii
usa_map <- map_data("world", regions = "USA")
projection <- "+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=37.5 +lon_0=-96"
# Create the base map with Alaska and Hawaii
gg <- ggplot() +
geom_polygon(data = usa_map, aes(x = long, y = lat, group = group), fill = "gray90", colour = "white") +
coord_map(projection = projection) +
theme_minimal() +
labs(title = "Coverage of Data Across the USA",
subtitle = "Includes Continental US, Alaska, and Hawaii")
# Add your data points
gg <- gg + geom_point(data = data, aes(x = Longitude, y = Latitude), colour = "red", size = 2, alpha = 0.6)
# Print the map
print(gg)
library(ggplot2)
library(maps)
library(mapproj)
# Example data frame with Longitude and Latitude
# Ensure to replace this with your actual dataset
data <- data.frame(
Longitude = c(-149.9003, -155.6659, -77.0369),  # Example: Alaska, Hawaii, and Washington D.C.
Latitude = c(61.2181, 19.8968, 38.9072)
)
# Get map data for the USA including Alaska and Hawaii
usa_map <- map_data("world", regions = "USA")
projection <- "+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=37.5 +lon_0=-96"
# Create the base map with Alaska and Hawaii
gg <- ggplot() +
geom_polygon(data = usa_map, aes(x = long, y = lat, group = group), fill = "gray90", colour = "white") +
coord_map(projection = projection) +
theme_minimal() +
labs(title = "Coverage of Data Across the USA",
subtitle = "Includes Continental US, Alaska, and Hawaii")
# Add your data points
gg <- gg + geom_point(data = data, aes(x = Longitude, y = Latitude), colour = "red", size = 2, alpha = 0.6)
# Print the map
print(gg)
# Load the libraries
library(ggplot2)
library(maps)
# Example data frame with Longitude and Latitude
# Ensure to replace this with your actual dataset
data <- data.frame(
Longitude = c(-149.9003, -155.6659, -77.0369),  # Example: Alaska, Hawaii, and Washington D.C.
Latitude = c(61.2181, 19.8968, 38.9072)
)
# Get map data for the USA including Alaska and Hawaii
usa_map <- map_data("usa")
# Create the base map
gg <- ggplot() +
geom_polygon(data = usa_map, aes(x = long, y = lat, group = group), fill = "gray90", colour = "black") +
theme_minimal() +
labs(title = "Coverage of Data Across the USA",
subtitle = "Includes Continental US, Alaska, and Hawaii")
# Add your data points
gg <- gg + geom_point(data = data, aes(x = Longitude, y = Latitude), colour = "red", size = 2, alpha = 0.6)
# Adjust the limits to include Alaska and Hawaii
gg <- gg + coord_fixed(ratio = 1.3, xlim = c(-190, -60), ylim = c(20, 75))
# Print the map
print(gg)
# Load necessary libraries
library(ggplot2)
library(maps)
library(mapproj)  # For more sophisticated map projections
# Load your data with latitude and longitude
# Example loading process - replace with actual loading code
data <- read.csv("path_to_your_data.csv")
# Load necessary libraries
library(ggplot2)
library(maps)
library(mapproj)  # For more sophisticated map projections
# Load your data with latitude and longitude
# Example loading process - replace with actual loading code
# Base map with modified projection for better visualization
usa_map <- map_data("world", regions = "USA")
gg <- ggplot() +
geom_polygon(data = usa_map, aes(x = long, y = lat, group = group),
fill = "gray90", colour = "black") +
coord_fixed(1.3) + # Fix the aspect ratio
theme_minimal() +
labs(title = "Coverage of Data Across the USA",
subtitle = "Includes Continental US, Alaska, and Hawaii")
# Add data points
gg <- gg + geom_point(data = data, aes(x = Longitude, y = Latitude),
color = "red", alpha = 0.6, size = 1)
# Modify the coordinate system to include Alaska and Hawaii
gg <- gg + coord_map("albers", lat0 = 39, lat1 = 45)
# Print the map
print(gg)
unique_pairs_count <- data %>%
distinct(Longitude, Latitude) %>%
nrow()  # Get the number of unique rows
# Print the count of unique pairs
print(unique_pairs_count)
unique_pairs_count <- data %>%
distinct(Longitude, Latitude) %>%
nrow()  # Get the number of unique rows
# Print the count of unique pairs
print(unique_pairs_count)
data(10
)
data
data<- read_csv("AT&T Public_Climate_Data_2022/Wind_Extremes_Historic_and_Midcentury.csv")
summary(data)
nrow(data)
# Load necessary libraries
library(ggplot2)
library(maps)
library(mapproj)  # For more sophisticated map projections
# Load your data with latitude and longitude
# Example loading process - replace with actual loading code
# Base map with modified projection for better visualization
usa_map <- map_data("world", regions = "USA")
gg <- ggplot() +
geom_polygon(data = usa_map, aes(x = long, y = lat, group = group),
fill = "gray90", colour = "black") +
coord_fixed(1.3) + # Fix the aspect ratio
theme_minimal() +
labs(title = "Coverage of Data Across the USA",
subtitle = "Includes Continental US, Alaska, and Hawaii")
# Add data points
gg <- gg + geom_point(data = data, aes(x = Longitude, y = Latitude),
color = "red", alpha = 0.6, size = 1)
# Modify the coordinate system to include Alaska and Hawaii
gg <- gg + coord_map("albers", lat0 = 39, lat1 = 45)
# Print the map
print(gg)
unique_pairs_count <- data %>%
distinct(Longitude, Latitude) %>%
nrow()  # Get the number of unique rows
# Print the count of unique pairs
print(unique_pairs_count)
# Load the libraries
library(ggplot2)
library(maps)
# Get map data for the USA including Alaska and Hawaii
usa_map <- map_data("usa")
# Create the base map
gg <- ggplot() +
geom_polygon(data = usa_map, aes(x = long, y = lat, group = group), fill = "gray90", colour = "black") +
theme_minimal() +
labs(title = "Coverage of Data Across the USA",
subtitle = "Includes Continental US, Alaska, and Hawaii")
# Add your data points
gg <- gg + geom_point(data = data, aes(x = Longitude, y = Latitude), colour = "red", size = 2, alpha = 0.6)
# Adjust the limits to include Alaska and Hawaii
gg <- gg + coord_fixed(ratio = 1.3, xlim = c(-190, -60), ylim = c(20, 75))
# Print the map
print(gg)
library(ggplot2)
library(albersusa)
library(ggplot2)
install.packages("albersusa", repos = "http://cran.us.r-project.org")
library(albersusa)
# Load necessary libraries
library(ggplot2)
library(maps)
# Assuming data is loaded with Latitude and Longitude columns
# data <- read.csv("your_data.csv")
# Get basic USA map data
usa_map <- map_data("usa")
# Create the plot
ggplot() +
geom_polygon(data = usa_map, aes(x = long, y = lat, group = group), fill = "gray80", colour = "black") +
geom_point(data = data, aes(x = Longitude, y = Latitude), colour = "red", size = 1, alpha = 0.6) +
coord_quickmap() +
labs(title = "Data Coverage Map", subtitle = "Geographic distribution of data points across the USA") +
theme_minimal()
data <- read_excel("AT&T Public_Climate_Data_2022/Drought_Extremes_Historic_and_Midcentury.xlsx")
summary(data)
nrow(data)
# Alternatively, for a more detailed exploration, you can use dplyr to summarise each colum
library(readr)
data <- read_csv("AT&T Public_Climate_Data_2022/Wildfire_Extremes_Historic_and_Midcentury.csv")
summary(data)
nrow(data)
library(readxl)
X1_0_communities <- read_excel("data/1.0-communities.xlsx")
colnames(X1_0_communities)
# Load necessary libraries
library(readxl)
library(dplyr)
library(ggplot2)
library(sf)  # For handling spatial data
# Load your Excel data
X1_0_communities <- read_excel("data/1.0-communities.xlsx")
# Load necessary libraries
library(readxl)
library(dplyr)
library(ggplot2)
library(sf)  # For handling spatial data
# Load your Excel data
X1_0_communities <- read_excel("data/1.0-communities.xlsx")
# Load the shapefile for census tracts
tracts_shapefile <- st_read("path_to_shapefile/tracts_shape.shp")
# Load necessary libraries
library(readxl)
library(dplyr)
library(ggplot2)
library(sf)  # For handling spatial data
# Load your Excel data
X1_0_communities <- read_excel("data/1.0-communities.xlsx")
colnames(X1_0_communities)
# Getting unique values for 'County Name' and 'State/Territory'
unique_counties = X1_0_communities['County Name'].unique()
# Getting unique values for 'County Name' and 'State/Territory'
unique_counties = X1_0_communities['County Name'].unique()
# Getting unique values for 'County Name' and 'State/Territory'
unique_counties = X1_0_communities['County Name'].unique()
unique_counties <- unique(X1_0_communities$`County Name`)
unique_states <- unique(X1_0_communities$`State/Territory`)
# Getting the count of unique values for 'County Name' and 'State/Territory'
count_counties <- length(unique(X1_0_communities$`County Name`))
count_states <- length(unique(X1_0_communities$`State/Territory`))
# Print the results
print("Unique Counties:")
print(unique_counties)
print("Unique States/Territories:")
print(unique_states)
print("Number of Unique Counties:")
print(count_counties)
print("Number of Unique States/Territories:")
print(count_states)
income_by_county <- X1_0_communities %>%
group_by(`County Name`) %>%
summarise(
Total_Tracts = n(),  # Count total tracts in each county
Low_Income_Tracts = sum(`Is low income?`, na.rm = TRUE),  # Count tracts where Is low income? is TRUE
Percentage_Low_Income = (Low_Income_Tracts / Total_Tracts) * 100  # Calculate percentage
)
# Print the results
print(income_by_county)
percentage_high_low_income_counties <- income_by_county %>%
filter(Percentage_Low_Income > 50) %>%  # Filter counties with more than 50% low income tracts
summarise(
Count_High_Low_Income = n(),  # Count these counties
Total_Counties = n_distinct(`County Name`)  # Count all counties for accuracy
) %>%
mutate(
Percentage = (Count_High_Low_Income / Total_Counties) * 100  # Calculate the percentage
)
# Print the final result
print(percentage_high_low_income_counties)
income_by_county <- X1_0_communities %>%
group_by(`County Name`) %>%
summarise(
Total_Tracts = n(),  # Count total tracts in each county
Low_Income_Tracts = sum(`Is low income?`, na.rm = TRUE),  # Count tracts where Is low income? is TRUE
Percentage_Low_Income = (Low_Income_Tracts / Total_Tracts) * 100  # Calculate percentage
)
# Print the results
print(income_by_county)
percentage_high_low_income_counties <- income_by_county %>%
filter(Percentage_Low_Income > 50) %>%  # Filter counties with more than 50% low income tracts
summarise(
Count_High_Low_Income = n(),  # Count these counties
Total_Counties = n_distinct(`County Name`)  # Count all counties for accuracy
) %>%
mutate(
Percentage = (Count_High_Low_Income / Total_Counties) * 100  # Calculate the percentage
)
# Print the final result
print(percentage_high_low_income_counties)
income_by_county <- X1_0_communities %>%
group_by(`County Name`) %>%
summarise(
Total_Tracts = n(),  # Count total tracts in each county
Low_Income_Tracts = sum(`Is low income?`, na.rm = TRUE),  # Count tracts where Is low income? is TRUE
Percentage_Low_Income = (Low_Income_Tracts / Total_Tracts) * 100  # Calculate percentage
)
# Print the results
print(income_by_county)
# Calculate the percentage of counties where more than 50% of tracts are low income
percentage_high_low_income_counties <- income_by_county %>%
filter(Percentage_Low_Income > 50) %>%  # Filter counties with more than 50% low income tracts
summarise(
Count_High_Low_Income = n(),  # Count these counties
Total_Counties = n_distinct(`County Name`)  # Count all counties for accuracy
) %>%
mutate(
Percentage = (Count_High_Low_Income / max(Total_Counties)) * 100  # Calculate the percentage
)
# Print the final result
print(percentage_high_low_income_counties)
library(dplyr)
# Assuming income_by_county already exists and contains the necessary data.
# First, calculate the total number of distinct counties in the entire dataset.
total_counties <- n_distinct(income_by_county$`County Name`)
# Calculate the percentage of counties where more than 50% of tracts are low income
percentage_high_low_income_counties <- income_by_county %>%
filter(Percentage_Low_Income > 50) %>%  # Filter counties with more than 50% low income tracts
summarise(
Count_High_Low_Income = n()  # Count these counties
) %>%
mutate(
Total_Counties = total_counties,  # Use the total number of counties calculated earlier
Percentage = (Count_High_Low_Income / Total_Counties) * 100  # Calculate the percentage
)
# Print the final result
print(percentage_high_low_income_counties)
life_expectancy_summary <- summary(data$`Life expectancy (years)`)
life_expectancy_summary <- summary(X1_0_communities$`Life expectancy (years)`)
asthma_summary <- summary(X1_0_communities$`Current asthma among adults aged greater than or equal to 18 years`)
diabetes_summary <- summary(X1_0_communities$`Diagnosed diabetes among adults aged greater than or equal to 18 years`)
heart_disease_summary <- summary(X1_0_communities$`Coronary heart disease among adults aged greater than or equal to 18 years`)
# Print the summaries
print("Summary of Life Expectancy (years):")
print(life_expectancy_summary)
print("Summary of Current Asthma among Adults:")
print(asthma_summary)
print("Summary of Diagnosed Diabetes among Adults:")
print(diabetes_summary)
print("Summary of Coronary Heart Disease among Adults:")
print(heart_disease_summary)
sum(is.na(X1_0_communities$`Is low income?`))
unique_ID <- unique(X1_0_communities$`Census tract 2010 ID`)
count_ID <- length(unique(X1_0_communities$`Census tract 2010 ID`))
# Print the results
print("Unique ID:")
print(unique_ID)
print("Number of Unique ID:")
print(count_ID)
# Load necessary libraries
library(readxl)
library(dplyr)
library(ggplot2)
library(sf)  # For handling spatial data
# Load your Excel data
X1_0_communities <- read_excel("data/1.0-communities.xlsx")
low_income_counts <- X1_0_communities %>%
count(is_low_income)
low_income_counts <- X1_0_communities %>%
count(`Is low income?`)
# Display the result
low_income_counts
summary(X1_0_communities$`Total population`)
